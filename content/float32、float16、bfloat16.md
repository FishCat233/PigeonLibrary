Float32：float point 32，又称FP32。32位浮点数。
Float16：FP16，16位浮点数
Bfloat16：BF16，指数位和FP32相同，但精度逼FP16差

|  类型  |  指数位   |   尾数位   |
| :--: | :----: | :-----: |
| FP32 | 8 bits | 23 bits |
| FP16 | 5 bits | 10 bits |
| BF16 | 8 bits | 7 bits  |

FP32称为全精度（4字节），FP16和BF16为半精度（2字节）

**混合精度训练**：指的是使用FP32作为主权重，而在进行前向和后向传播时使用FP16/BF16来提升训练速度，最后在梯度更新阶段再使用FP16/BF16梯度更新FP32主权重。